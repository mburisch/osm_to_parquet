{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd118c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47135/2524390554.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Generator\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from tqdm.autonotebook import tqdm\n",
    "import more_itertools as mit\n",
    "from pathlib import Path\n",
    "from collections.abc import Iterable\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def local_spark(config: dict[str, str] | None = None) -> Generator[SparkSession, None, None]:\n",
    "    config = config or {}\n",
    "    try:\n",
    "        builder = SparkSession.builder\n",
    "        builder.enableHiveSupport()\n",
    "        builder.appName(\"local_spark\")\n",
    "        for key, value in config.items():\n",
    "            builder.config(key, value)\n",
    "        \n",
    "        spark = builder.getOrCreate()\n",
    "        yield spark\n",
    "    finally:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091204b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "#from protos.fileformat_pb2 import Blob, BlobHeader\n",
    "#from protos.osmformat_pb2 import HeaderBlock, PrimitiveBlock\n",
    "\n",
    "@dataclass\n",
    "class BlobData:\n",
    "    header: BlobHeader\n",
    "    blob: Blob\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RawBlobData:\n",
    "    blob_type: str\n",
    "    data: bytes\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OsmInfo:\n",
    "    version: int\n",
    "    timestamp: int\n",
    "    changeset: int\n",
    "    uid: int\n",
    "    user_sid: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OsmNode:\n",
    "    id: int\n",
    "    info: OsmInfo\n",
    "    tags: dict[str, str]\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OsmWay:\n",
    "    id: int\n",
    "    info: OsmInfo\n",
    "    tags: dict[str, str]\n",
    "    nodes: list[int]\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OsmRelationMember:\n",
    "    id: int\n",
    "    role: str\n",
    "    type: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OsmRelation:\n",
    "    id: int\n",
    "    info: OsmInfo\n",
    "    tags: dict[str, str]\n",
    "    members: list[OsmRelationMember]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OsmData:\n",
    "    nodes: list[OsmNode]\n",
    "    ways: list[OsmWay]\n",
    "    relations: list[OsmRelation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41dfdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "def decompress_blob(blob: Blob) -> bytes:\n",
    "    match blob.WhichOneof(\"data\"):\n",
    "        case \"raw\":\n",
    "            return blob.raw\n",
    "        case \"zlib_data\":\n",
    "            return zlib.decompress(blob.zlib_data)\n",
    "        case _:\n",
    "            raise ValueError(\"Blob has no data\")\n",
    "\n",
    "\n",
    "def decode_blob(header: BlobHeader, blob: Blob) -> HeaderBlock | PrimitiveBlock:\n",
    "    data = decompress_blob(blob)\n",
    "    match header.type:\n",
    "        case \"OSMHeader\":\n",
    "            return HeaderBlock.FromString(data)\n",
    "        case \"OSMData\":\n",
    "            return PrimitiveBlock.FromString(data)\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown blob type: {header.type}\")\n",
    "\n",
    "\n",
    "\n",
    "def decode_blob_data(blob_data: BlobData) -> HeaderBlock | PrimitiveBlock:\n",
    "    block = decode_blob(blob_data.header, blob_data.blob)\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c099ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from protos.osmformat_pb2 import Way, Node, Relation, DenseNodes, Info, DenseInfo\n",
    "\n",
    "\n",
    "def delta_decode(values: Iterable[int]) -> list[int]:\n",
    "    result = []\n",
    "    current = 0\n",
    "    for value in values:\n",
    "        current += value\n",
    "        result.append(current)\n",
    "    return result\n",
    "\n",
    "\n",
    "class ValueDecoder:\n",
    "    def __init__(self, block: PrimitiveBlock) -> None:\n",
    "        self.granularity = block.granularity or 100\n",
    "        self.lat_offset = block.lat_offset or 0\n",
    "        self.lon_offset = block.lon_offset or 0\n",
    "        self.date_granularity = block.date_granularity or 1000\n",
    "\n",
    "    def lat(self, value: int) -> float:\n",
    "        return 0.000000001 * (self.lat_offset + (self.granularity * value))\n",
    "\n",
    "    def lon(self, value: int) -> float:\n",
    "        return 0.000000001 * (self.lon_offset + (self.granularity * value))\n",
    "\n",
    "    def timestamp(self, value: int) -> int:\n",
    "        return value * self.date_granularity\n",
    "\n",
    "\n",
    "class PrimitiveBlockDecoder:\n",
    "    def __init__(self, block: PrimitiveBlock) -> None:\n",
    "        self.block = block\n",
    "        self.string_table = [s.decode(\"utf-8\") for s in block.stringtable.s]\n",
    "        self.value_decoder = ValueDecoder(block)\n",
    "\n",
    "    def decode_string(self, index: int) -> str:\n",
    "        return self.string_table[index]\n",
    "\n",
    "    def decode_info(self, info: Info) -> OsmInfo:\n",
    "        return OsmInfo(\n",
    "            version=info.version,\n",
    "            timestamp=info.timestamp,\n",
    "            changeset=info.changeset,\n",
    "            uid=info.uid,\n",
    "            user_sid=self.decode_string(info.user_sid),\n",
    "        )\n",
    "\n",
    "    def decode_tags(self, keys: list[int], vals: list[int]) -> dict[str, str]:\n",
    "        keys = (self.decode_string(key) for key in keys)\n",
    "        vals = (self.decode_string(val) for val in vals)\n",
    "        return dict(zip(keys, vals))\n",
    "\n",
    "    def decode_node(self, node: Node) -> OsmNode:\n",
    "        return OsmWay(\n",
    "            id=node.id,\n",
    "            info=self.decode_info(node.info),\n",
    "            tags=self.decode_tags(node.keys, node.vals),\n",
    "            latitude=self.value_decoder(node.lat),\n",
    "            longitude=self.value_decoder(node.lon),\n",
    "        )\n",
    "\n",
    "    def decode_dense_info(self, dense: DenseInfo) -> list[OsmInfo]:\n",
    "        return [\n",
    "            OsmInfo(\n",
    "                version=version,\n",
    "                timestamp=timestamp,\n",
    "                changeset=changeset,\n",
    "                uid=uid,\n",
    "                user_sid=self.decode_string(user_sid),\n",
    "            )\n",
    "            for version, timestamp, changeset, uid, user_sid in zip(\n",
    "                dense.version,\n",
    "                delta_decode(dense.timestamp),\n",
    "                delta_decode(dense.changeset),\n",
    "                delta_decode(dense.uid),\n",
    "                delta_decode(dense.user_sid),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def decode_dense_nodes(self, dense: DenseNodes) -> list[OsmNode]:\n",
    "        tags = self.decode_tags(*mit.distribute(2, dense.keys_vals))\n",
    "        return [\n",
    "            OsmNode(\n",
    "                id=id,\n",
    "                info=info,\n",
    "                tags=tags,\n",
    "                latitude=lat,\n",
    "                longitude=lon,\n",
    "            )\n",
    "            for id, info, lat, lon in zip(\n",
    "                delta_decode(dense.id),\n",
    "                self.decode_dense_info(dense.denseinfo),\n",
    "                delta_decode(dense.lat),\n",
    "                delta_decode(dense.lon),\n",
    "                \n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def decode_way(self, way: Way) -> OsmWay:\n",
    "        return OsmWay(\n",
    "            id=way.id,\n",
    "            info=self.decode_info(way.info),\n",
    "            tags=self.decode_tags(way.keys, way.vals),\n",
    "            nodes=delta_decode(way.refs),\n",
    "        )\n",
    "\n",
    "    def decode_relation(self, relation: Relation) -> OsmRelation:\n",
    "        return OsmRelation(\n",
    "            id=relation.id,\n",
    "            info=self.decode_info(relation.info),\n",
    "            tags=self.decode_tags(relation.keys, relation.vals),\n",
    "            members=[\n",
    "                OsmRelationMember(\n",
    "                    id=member_id,\n",
    "                    role=self.decode_string(role_sid),\n",
    "                    type=Relation.MemberType.Name(member_type),\n",
    "                )\n",
    "                for role_sid, member_id, member_type in zip(\n",
    "                    relation.roles_sid, delta_decode(relation.memids), relation.types\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def decode_block(block: PrimitiveBlock) -> OsmData:\n",
    "    decoder = PrimitiveBlockDecoder(block)\n",
    "    nodes = [\n",
    "        decoder.decode_node(node)\n",
    "        for group in block.primitivegroup\n",
    "        for node in group.nodes\n",
    "    ]\n",
    "    nodes += list(mit.flatten(decoder.decode_dense_nodes(group.dense) for group in block.primitivegroup))\n",
    "    ways = [decoder.decode_way(way) for group in block.primitivegroup for way in group.ways]\n",
    "    relations = [decoder.decode_relation(relation) for group in block.primitivegroup for relation in group.relations]\n",
    "    return OsmData(nodes=nodes, ways=ways, relations=relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cae22b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "\n",
    "def read_blob_data(source: BytesIO) -> BlobData | None:\n",
    "    data = source.read(4)\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "\n",
    "    header_size = int.from_bytes(data, \"big\")\n",
    "\n",
    "    data = source.read(header_size)\n",
    "    blob_header = BlobHeader.FromString(data)\n",
    "\n",
    "    data = source.read(blob_header.datasize)\n",
    "    blob = Blob.FromString(data)\n",
    "\n",
    "    return BlobData(header=blob_header, blob=blob)\n",
    "\n",
    "\n",
    "def read_raw_blob_data(source: BytesIO) -> RawBlobData | None:\n",
    "    from protos.fileformat_pb2 import BlobHeader\n",
    "\n",
    "    data = source.read(4)\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "\n",
    "    header_size = int.from_bytes(data, \"big\")\n",
    "\n",
    "    data = source.read(header_size)\n",
    "    blob_header = BlobHeader.FromString(data)\n",
    "    data = source.read(blob_header.datasize)\n",
    "    \n",
    "    return RawBlobData(blob_type=str(blob_header.type), data=data)\n",
    "\n",
    "\n",
    "def read_blobs(source: BytesIO) -> Generator[BlobData, None, None]:\n",
    "    while True:\n",
    "        data = read_blob_data(source)\n",
    "        if data is None:\n",
    "            return\n",
    "        if data.header.type == \"OSMHeader\":\n",
    "            continue\n",
    "        yield data\n",
    "        \n",
    "\n",
    "\n",
    "def read_raw_blobs(source: BytesIO) -> Generator[RawBlobData, None, None]:\n",
    "    while True:\n",
    "        data = read_raw_blob_data(source)\n",
    "        if data is None:\n",
    "            return\n",
    "        yield data\n",
    "        \n",
    "\n",
    "# def read_pbf(filename: Path) -> None:\n",
    "#     stats = dict(nodes=0, ways=0, relations=0)\n",
    "#     with open(filename, \"rb\") as fin:\n",
    "#         for data in tqdm(read_blobs(fin)):\n",
    "#             block = decode_blob_data(data)\n",
    "#             nodes, ways, relations = decode_block(block)\n",
    "#             stats[\"nodes\"] += len(nodes)\n",
    "#             stats[\"ways\"] += len(ways)\n",
    "#             stats[\"relations\"] += len(relations)\n",
    "\n",
    "#     return stats\n",
    "# #read_pbf(Path(\"/workspaces/data/osm/us-latest.osm.pbf\"))\n",
    "# read_pbf(Path(\"/workspaces/data/osm/nevada-latest.osm.pbf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cab7ee63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/10 22:06:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py:161: DeprecationWarning: This process (pid=47489) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "25/07/10 22:06:24 ERROR Utils: uncaught error in thread Spark Context Cleaner, stopping SparkContext\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle.allocateInstance(DirectMethodHandle.java:501)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.newInvokeSpecial(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:196)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda/0x00007458685548d0.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1279)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:190)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:80)\n",
      "25/07/10 22:06:24 ERROR Utils: throw uncaught fatal error in thread Spark Context Cleaner\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle.allocateInstance(DirectMethodHandle.java:501)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.newInvokeSpecial(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:196)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda/0x00007458685548d0.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1279)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:190)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:80)\n",
      "Exception in thread \"Spark Context Cleaner\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle.allocateInstance(DirectMethodHandle.java:501)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.newInvokeSpecial(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:196)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda/0x00007458685548d0.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1279)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:190)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:80)\n",
      "25/07/10 22:06:24 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)\n",
      "java.lang.OutOfMemoryError: Java heap space: failed reallocation of scalar replaced objects\n",
      "\tat java.base/java.lang.String.encodeUTF8(String.java:1303)\n",
      "\tat java.base/java.lang.String.encode(String.java:867)\n",
      "\tat java.base/java.lang.String.getBytes(String.java:1818)\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromString(UTF8String.java:184)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$11$1.applyOrElse(EvaluatePython.scala:149)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:221)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$11(EvaluatePython.scala:148)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$$Lambda/0x0000745868f76368.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$14$1.$anonfun$applyOrElse$3(EvaluatePython.scala:175)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$14$1$$Lambda/0x000074586902d3e8.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapData$.apply(ArrayBasedMapData.scala:65)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$14$1.applyOrElse(EvaluatePython.scala:176)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:221)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$14(EvaluatePython.scala:171)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$$Lambda/0x0000745868f76ae0.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:196)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:221)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$$Lambda/0x0000745868f76720.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.$anonfun$applyOrElse$1(EvaluatePython.scala:162)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1$$Lambda/0x0000745869027aa0.apply(Unknown Source)\n",
      "\tat scala.collection.StrictOptimizedIterableOps.map(StrictOptimizedIterableOps.scala:100)\n",
      "\tat scala.collection.StrictOptimizedIterableOps.map$(StrictOptimizedIterableOps.scala:87)\n",
      "\tat scala.collection.convert.JavaCollectionWrappers$JListWrapper.map(JavaCollectionWrappers.scala:138)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.applyOrElse(EvaluatePython.scala:162)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:221)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$13(EvaluatePython.scala:160)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$$Lambda/0x0000745868f77258.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:196)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:221)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$$Lambda/0x0000745868f76720.apply(Unknown Source)\n",
      "25/07/10 22:06:24 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#157,Executor task launch worker for task 0.0 in stage 0.0 (TID 0),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space: failed reallocation of scalar replaced objects\n",
      "\tat java.base/java.lang.String.encodeUTF8(String.java:1303)\n",
      "\tat java.base/java.lang.String.encode(String.java:867)\n",
      "\tat java.base/java.lang.String.getBytes(String.java:1818)\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromString(UTF8String.java:184)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$11$1.applyOrElse(EvaluatePython.scala:149)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:221)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$11(EvaluatePython.scala:148)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$$Lambda/0x0000745868f76368.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$14$1.$anonfun$applyOrElse$3(EvaluatePython.scala:175)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$14$1$$Lambda/0x000074586902d3e8.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapData$.apply(ArrayBasedMapData.scala:65)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$14$1.applyOrElse(EvaluatePython.scala:176)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:221)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$14(EvaluatePython.scala:171)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$$Lambda/0x0000745868f76ae0.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:196)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:221)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$$Lambda/0x0000745868f76720.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.$anonfun$applyOrElse$1(EvaluatePython.scala:162)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1$$Lambda/0x0000745869027aa0.apply(Unknown Source)\n",
      "\tat scala.collection.StrictOptimizedIterableOps.map(StrictOptimizedIterableOps.scala:100)\n",
      "\tat scala.collection.StrictOptimizedIterableOps.map$(StrictOptimizedIterableOps.scala:87)\n",
      "\tat scala.collection.convert.JavaCollectionWrappers$JListWrapper.map(JavaCollectionWrappers.scala:138)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.applyOrElse(EvaluatePython.scala:162)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:221)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$13(EvaluatePython.scala:160)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$$Lambda/0x0000745868f77258.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:196)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:221)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$$Lambda/0x0000745868f76720.apply(Unknown Source)\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 42516)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/current/lib/python3.13/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/current/lib/python3.13/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/current/lib/python3.13/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/current/lib/python3.13/socketserver.py\", line 766, in __init__\n",
      "    self.handle()\n",
      "    ~~~~~~~~~~~^^\n",
      "  File \"/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "    ~~~~^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ~~~~^^\n",
      "  File \"/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mlocal_spark\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m     26\u001b[39m     spark = builder.getOrCreate()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m spark\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m    100\u001b[39m df = df.select(F.explode(\u001b[33m\"\u001b[39m\u001b[33minfo.nodes\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mnode\u001b[39m\u001b[33m\"\u001b[39m)) \n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:285\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:303\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31m<class 'str'>\u001b[39m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPy4JNetworkError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/py4j/clientserver.py:540\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer.strip() == \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[32m    541\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAnswer from Java side is empty\u001b[39m\u001b[33m\"\u001b[39m, when=proto.EMPTY_RESPONSE)\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer.startswith(proto.RETURN_MESSAGE):\n",
      "\u001b[31mPy4JNetworkError\u001b[39m: Answer from Java side is empty",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 104\u001b[39m\n\u001b[32m    100\u001b[39m         df = df.select(F.explode(\u001b[33m\"\u001b[39m\u001b[33minfo.nodes\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mnode\u001b[39m\u001b[33m\"\u001b[39m)) \n\u001b[32m    102\u001b[39m         df.show()\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/workspaces/data/osm/nevada-latest.osm.pbf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m local_spark() \u001b[38;5;28;01mas\u001b[39;00m spark_session:\n\u001b[32m     95\u001b[39m         spark_session.dataSource.register(PbfDataSource)\n\u001b[32m     96\u001b[39m         df = spark_session.read.format(\u001b[33m\"\u001b[39m\u001b[33mpbf\u001b[39m\u001b[33m\"\u001b[39m).option(\u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m,filename).load()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/current/lib/python3.13/contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    160\u001b[39m     value = typ()\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mlocal_spark\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m spark\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/pyspark/sql/session.py:2026\u001b[39m, in \u001b[36mSparkSession.stop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2024\u001b[39m \u001b[38;5;66;03m# We should clean the default session up. See SPARK-23228.\u001b[39;00m\n\u001b[32m   2025\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2026\u001b[39m jSparkSessionClass = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_j_spark_session_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2027\u001b[39m jSparkSessionClass.clearDefaultSession()\n\u001b[32m   2028\u001b[39m jSparkSessionClass.clearActiveSession()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/pyspark/sql/session.py:668\u001b[39m, in \u001b[36mSparkSession._get_j_spark_session_class\u001b[39m\u001b[34m(jvm)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_j_spark_session_class\u001b[39m(jvm: \u001b[33m\"\u001b[39m\u001b[33mJVMView\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mJavaClass\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.spark.sql.classic.SparkSession\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1752\u001b[39m, in \u001b[36mJVMView.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1749\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == UserHelpAutoCompletion.KEY:\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[32m-> \u001b[39m\u001b[32m1752\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1753\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1754\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1755\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer == proto.SUCCESS_PACKAGE:\n\u001b[32m   1757\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m._gateway_client, jvm_id=\u001b[38;5;28mself\u001b[39m._id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1057\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1055\u001b[39m         retry = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1056\u001b[39m     logging.info(\u001b[33m\"\u001b[39m\u001b[33mException while sending command.\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1059\u001b[39m     logging.exception(\n\u001b[32m   1060\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mException while sending command.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/osm_to_parquet/.venv/lib/python3.13/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from collections.abc import Iterator\n",
    "from pyspark.sql.datasource import DataSource, DataSourceReader, InputPartition\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, MapType, ArrayType, DoubleType, LongType\n",
    "\n",
    "\n",
    "\n",
    "ReadState = dict[str, Any]\n",
    "\n",
    "class PbfDataSource(DataSource):\n",
    "    @classmethod\n",
    "    def name(cls) -> str:\n",
    "        return \"pbf\"\n",
    "\n",
    "    def schema(self) -> str:\n",
    "        return \"header_type string, data binary\"\n",
    "\n",
    "    def reader(self, schema: StructType) -> PbfDataSourceReader:\n",
    "        return PbfDataSourceReader(schema, self.options)\n",
    "    \n",
    "\n",
    "class PbfDataSourceReader(DataSourceReader):\n",
    "    def __init__(self, schema: StructType, options: dict[str, Any]) -> None:\n",
    "        self.schema = schema\n",
    "        self.options = options\n",
    "\n",
    "    def partitions(self) -> list[InputPartition]:\n",
    "        return [InputPartition(0)]\n",
    "\n",
    "    def read(self, partition: InputPartition) -> Iterator[tuple[str, bytes]]:\n",
    "        with open(self.options[\"filename\"], \"rb\") as fin:\n",
    "            for data in read_raw_blobs(fin):\n",
    "                yield str(data.blob_type), data.data\n",
    "\n",
    "\n",
    "TagType = MapType(StringType(), StringType())\n",
    "\n",
    "InfoType = StructType([\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"timestamp\", LongType()),\n",
    "    StructField(\"changeset\", LongType()),\n",
    "    StructField(\"uid\", LongType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "])\n",
    "\n",
    "NodeType = StructType([\n",
    "    StructField(\"id\", LongType()),\n",
    "    StructField(\"info\", InfoType),\n",
    "    StructField(\"tags\", TagType),\n",
    "    StructField(\"latitude\", DoubleType()),\n",
    "    StructField(\"longitude\", DoubleType()),\n",
    "])\n",
    "\n",
    "WayType = StructType([\n",
    "    StructField(\"id\", LongType()),\n",
    "    StructField(\"info\", InfoType),\n",
    "    StructField(\"tags\", TagType),\n",
    "    StructField(\"nodes\", ArrayType(LongType())),\n",
    "])\n",
    "\n",
    "\n",
    "RelationType = StructType([\n",
    "    StructField(\"id\", LongType()),\n",
    "    StructField(\"info\", InfoType),\n",
    "    StructField(\"tags\", TagType),\n",
    "    StructField(\"members\", ArrayType(StructType([\n",
    "        StructField(\"id\", LongType()),\n",
    "        StructField(\"role\", StringType()),\n",
    "        StructField(\"type\", StringType()),\n",
    "    ]))),\n",
    "])\n",
    "\n",
    "\n",
    "OsmDataType = StructType([\n",
    "    StructField(\"nodes\", ArrayType(NodeType)),\n",
    "    StructField(\"ways\", ArrayType(WayType)),\n",
    "    StructField(\"relations\", ArrayType(RelationType)),\n",
    "])\n",
    "\n",
    "@F.udf(returnType=OsmDataType)\n",
    "def decode_data(data: bytes) -> tuple[str, int]:\n",
    "    from protos.fileformat_pb2 import Blob\n",
    "    from protos.osmformat_pb2 import PrimitiveBlock\n",
    "    from protos.osmformat_pb2 import Way, Node, Relation, DenseNodes, Info, DenseInfo\n",
    "\n",
    "    blob = Blob.FromString(data)\n",
    "    data = decompress_blob(blob)\n",
    "    block = PrimitiveBlock.FromString(data)\n",
    "    result = decode_block(block)\n",
    "\n",
    "    return result.nodes, result.ways, result.relations\n",
    "    \n",
    "\n",
    "def run(filename: str) -> None:\n",
    "    with local_spark() as spark_session:\n",
    "        spark_session.dataSource.register(PbfDataSource)\n",
    "        df = spark_session.read.format(\"pbf\").option(\"filename\",filename).load()\n",
    "        df = df.where(F.col(\"header_type\") == \"OSMData\")\n",
    "        df = df.repartition(100)\n",
    "        df = df.withColumn(\"info\", decode_data(\"data\"))\n",
    "        df = df.select(F.explode(\"info.nodes\").alias(\"node\")) \n",
    "\n",
    "        df.show()\n",
    "\n",
    "run(\"/workspaces/data/osm/nevada-latest.osm.pbf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1238a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26196ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osm_to_parquet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
